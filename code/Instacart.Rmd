---
title: "Instacart"
author: "Michael Fehl -- 730321206"
date: "`r Sys.Date()`"
output: html_document
---

# **Predicting Future Consumer Purchases**

### In this group research project, we are attempting to predict whether or not a customer reorders a given product based on their order history. In each order history, we have a breakdown of all the products purchased for that given order, including the pattern of which they put the item into their cart, the time of day they ordered, and the day of the week. 

### The data already had training data held out for our models to train on, and test data points kept to test our models on. However, we do not have access to these test data points unless we submit our models, which is not what we're trying to do. So instead, we will need to redefine the original training data set (1.4m obs) to become our test points, and the prior data set (30m obs) as our training data.

More information about the project & data can be found on github:
[Click here for more information](https://github.com/mtfehl/ECON573/tree/main)



# Importing Data & Data Exploration
===

```{r import & view, include=FALSE}
library(httr)
library(tidyverse)
library(ggthemes)
library(corrplot)
source("https://raw.githubusercontent.com/mtfehl/ECON573/main/data/ggplot_theme_Publication-2.R")
# Import CSV data from Github
depts <- read.csv("https://media.githubusercontent.com/media/mtfehl/ECON573/main/data/departments.csv")
aisles <- read.csv("https://media.githubusercontent.com/media/mtfehl/ECON573/main/data/aisles.csv")
order_train <- read.csv("https://media.githubusercontent.com/media/mtfehl/ECON573/main/data/order_products__train.csv")
products <- read.csv("https://media.githubusercontent.com/media/mtfehl/ECON573/main/data/products.csv")

# large file (3.4m obs)
response <- GET("https://media.githubusercontent.com/media/mtfehl/ECON573/main/data/orders.csv",
                timeout(300))

orders <- read.csv(text = content(response, "text"), header = TRUE,
                   encoding = "UTF-8")
# large file (30m+ obs)
response2 <- GET("https://media.githubusercontent.com/media/mtfehl/ECON573/main/data/order_products__prior.csv",
                timeout(600))
order_prior <- read.csv(text = content(response2, "text"), header = TRUE,
                        encoding = "UTF-8")

rm(response, response2)
```

# Initial Data Exploration

```{r data exploration}
# Data exploration
head(order_prior, 12) # Details of all the purchased products in a given order for each order_id
head(order_train, 12) # Subset of training data in same format as order_prior
head(orders, 12) # History of all orders for user and the associated order_id
head(products) # Names of all the products
head(aisles) # Names of all the aisles
head(depts) # Names of all the departments
```


# Data Manipulation & Visualization
===

``` {r manipulation & visualization}
# Response variable: reordered (1/0)
# We want to predict whether a given product is reordered or not by a customer
### put info and distribution of response here, its relationship w/ other variables

# Distribution of order by hour of day
ggplot(orders, aes(x = order_hour_of_day, fill = as.factor(order_hour_of_day))) + 
  geom_histogram(bins = 24) +
  labs(title = "Order Frequency by Hour",
       x = "Time of Day") +
  theme_Publication() + 
  theme(legend.position = "none")


# See how day of week variable is coded (numeric)
class(orders$order_dow)

# Changing DOW variable to be a factor variable with character labels
orders %>% 
  mutate(order_dow = factor(order_dow, labels = c("Saturday", "Sunday", "Monday",
                                                  "Tuesday", "Wednesday", "Thursday",
                                                  "Friday"))) %>% 
  ggplot(., aes(x = order_dow, fill = as.factor(order_dow))) + 
  geom_bar(width = 0.75, ) +
  labs(title = "Order Frequency by Day",
       x = " Day of the Week") +
  theme_Publication() +           # Distribution of order by day of the week
  scale_fill_Publication() + 
  theme(legend.position = "none")


# Distribution of order frequency
ggplot(orders, aes(x = days_since_prior_order, fill = as.factor(days_since_prior_order))) + 
geom_histogram(bins = 30) +
labs(title = "Frequency of Customer Ordering",
     x = " Days since Prior Order") +
theme_Publication() +     
theme(legend.position = "none") # We see most reorders happen bw/ 1-8 days. Interesting pattern -- we see a spike in reordering every 7 days. Finally, we see the largest single spike at 30 days -- not sure as if I can interpret this as just 30 days or maybe 30+ days.


# How many NA values are in our "days_since_prior_order" column
table(is.na(orders$days_since_prior_order)) # 206209

# Most ordered products? 
## subset of product data
product_freq_table <- order_prior %>% 
  count(product_id) %>% # count frequency of each product
  arrange(desc(n)) %>% # arrange in descending order
  inner_join(., products, by = "product_id") %>% # merge data sets to bring in associated product names
  select(-c(aisle_id, department_id)) %>%  # subset of only columns product_id, name, and frequency (n)
  slice(1:20) # access the 20 most frequent orders
## plot of most frequently ordered products
ggplot(product_freq_table, aes(x = reorder(product_name, -n), y = n, fill = as.factor(product_name))) + 
geom_bar(stat = "identity") +
labs(title = "Product Order Frequency",
     x = "Name of Product",
     y = "Total orders") +
theme_Publication() +  
scale_colour_Publication() +
theme(legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1))
```

# Merging Data
### First, I want to create a single data frame that combines all the order-specific information to the user-specific information, so that we can add more variables that are order-level and user-level into the same data frame, which will be useful for our classification models later.

``` {r create big data set}
# Create a full data set of all orders with all information on each order -- order_id, user_id, all products in that order, the associated aisle & department, etc.
full_order_data <- orders %>% 
  full_join(., order_train, by = "order_id") %>% 
  inner_join(., order_prior, by = "order_id") %>% 
  arrange(user_id, order_number) %>%  
  mutate(product_id = product_id.y,
         add_to_cart_order = add_to_cart_order.y,
         reordered = reordered.y,
         order_dow = factor(order_dow, 
                            levels = c(0, 1, 2, 3, 4, 5, 6),
                            labels = c("Saturday", "Sunday", "Monday", 
                                       "Tuesday", "Wednesday", "Thursday", "Friday"))) %>% 
  select(-c(product_id.x, add_to_cart_order.x, reordered.x,
            product_id.y, add_to_cart_order.y, reordered.y)) %>% 
  inner_join(., products, by = "product_id") %>% 
  inner_join(., aisles, by = "aisle_id") %>% 
  inner_join(., depts, by = "department_id") %>% 
  select(-c(aisle_id, department_id, product_id))

full_order_data[1:1000,] # looks good

rm(order_prior, order_train, orders)
```

# Variable Creation
## Now, we will be creating 9 new variables that will be used to explore further relationships in the data & added as part of the regression models.
===

## **New Variable 1:** 
### Frequency each Customer Orders a Given Product

``` {r New Variable 1}
# Now I want to create new variables
## First -- frequency each customer orders a given product
product_count_per_user <- full_order_data %>% 
  group_by(user_id, product_name) %>% 
  summarize(product_count = n()) %>% 
  arrange(user_id, desc(product_count))

```

## **New Variable 2:**
### Average time of day & day of the week each customer places an order

``` {r nv2}
## Average time of day & DOW customer places an order
avg_order_time <- full_order_data %>% 
  group_by(user_id) %>%
  summarize(avg_hour = mean(order_hour_of_day),
            avg_day = mean(as.numeric(order_dow)))

head(avg_order_time)
#plot -- not sure how to make this a heatmap
ggplot(avg_order_time, aes(x = avg_hour, y = avg_day)) +
  geom_point() +
  labs(x = "Average Hour of Day", y = "Average Day of Week",
       title = "Average Order Time") +
  scale_y_continuous(breaks = 0:6, 
                     labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
  theme_Publication()

```

## **New Variables 3 & 4:** 
### Which products are ordered more frequently based on the day of the week
### Which products are ordered more frequently based on the time of the day

``` {r nv3&4}
# What products are ordered more frequently based on DOW?
dow_freq_order <- full_order_data %>% 
  group_by(order_dow, product_name) %>%
  summarize(product_count_day = n()) %>% 
  arrange(order_dow, desc(product_count_day))

head(dow_freq_order)

# Top 3 products for each day of the week
dow_freq_order %>%
  group_by(order_dow) %>%
  top_n(3, product_count_day) %>% 
  ggplot(., aes(x = reorder(product_name, -product_count_day), y = product_count_day)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Product Name", y = "Product Count",
       title = "Top 3 Products Ordered by Day of Week") +
  theme_clean() +
  scale_fill_Publication() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ order_dow, nrow = 2, scales = "free_x")

# What products are ordered more frequently based on time of day?
hour_freq_order <- full_order_data %>%
  group_by(order_hour_of_day, product_name) %>%
  summarize(product_count_hour = n()) %>% 
  arrange(order_hour_of_day, desc(product_count_hour))

head(hour_freq_order)

# Top 3 products for each hour of the day
hour_freq_order %>%
  group_by(order_hour_of_day) %>%
  top_n(3, product_count_hour) %>% 
ggplot(., aes(x = reorder(product_name, -product_count_hour), y = product_count_hour)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Product Name", y = "Product Count",
       title = "Top 3 Products Ordered by Hour of Day") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) +
  facet_wrap(~ order_hour_of_day, nrow = 2, scales = "free_x")
```

## **New Variables 5 & 6:** 
### Size of each individual order
### Average order size per customer

``` {r nv5&6}
# Calculate order size
order_size <- full_order_data %>%
  group_by(order_id) %>%
  summarize(order_size = n())

head(order_size)

#plot

# Aggregate order size at the customer level and calculate average order size
avg_order_size <- order_size %>%
  inner_join(full_order_data, by = "order_id") %>%
  group_by(user_id) %>%
  summarize(avg_order_size = mean(order_size))

head(avg_order_size)
```

## **New Variables 7 & 8:** 
### Which products are reordered the most?
### User-level product reordering ratio

``` {r nv7&8}
# Which products are reordered the most?
reorder_counts <- full_order_data %>%
  group_by(product_name) %>%
  summarize(total_reorders = sum(reordered)) %>% 
  arrange(desc(total_reorders))

head(reorder_counts)
# plot
reorder_counts %>% 
  slice(1:20) %>% 
  ggplot(., aes(x = reorder(product_name, -total_reorders), y = total_reorders, 
                fill = as.factor(product_name))) + 
  geom_bar(stat = "identity") +
  labs(title = "Product Reorder Frequency",
     x = "Name of Product",
     y = "Total Reorders") +
  theme_Publication() +  
  scale_colour_Publication() +
  theme(legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1))

# Product reorder ratio, per user
product_reorder_ratio <- full_order_data %>%
  group_by(user_id, product_name) %>%
  summarize(product_reorder_ratio = sum(reordered) / n()) %>%
  arrange(user_id, desc(product_reorder_ratio))

product_reorder_ratio
# Distributions of all the variables. Try to replicate that cool graph of which products are linked

```

## **New Variable 9:**
### Reorder rate by item position in cart

``` {r nv9}
# What is the relationship between the order in which an item is added to the cart & reordering it?
reorder_rate_by_position <- full_order_data %>%
  group_by(add_to_cart_order) %>%
  summarize(reorder_rate = mean(reordered)) 

# plot
ggplot(reorder_rate_by_position, aes(x = add_to_cart_order, y = reorder_rate)) +
  geom_line() +
  geom_point() +
  labs(x = "Cart Position", y = "Reorder Rate", title = "Reorder Rate by Cart Position") +
  theme_Publication() +
  scale_fill_Publication()
# Logarithmic relationship until cart has ~50 items -- curious to see if these are just some outlier cases

# All different order sizes -- ranges from 1:145 items in a single order
range(full_order_data$add_to_cart_order)

# Filter observations for orders of over 50 items
full_order_data %>% 
  filter(add_to_cart_order > 50) %>% 
  group_by(order_id)
# Only 26,359 observations for 3,081 orders.. 
3081/32434489
# That's only 0.0095% of our total orders -- so we only care about orders below this threshold

# Redefine variable to filter only orders with 50 or less items in cart
reorder_rate_by_position <- reorder_rate_by_position %>% 
  filter(add_to_cart_order <= 50) 

# New plot -- looks much better. We now see a clear relationship b/w cart position & reordering
ggplot(reorder_rate_by_position, aes(x = add_to_cart_order, y = reorder_rate)) +
  geom_line() +
  geom_point() +
  labs(x = "Cart Position", y = "Reorder Rate", title = "Reorder Rate by Cart Position") +
  theme_Publication() +
  scale_fill_Publication()

head(reorder_rate_by_position)
```

## Merge New Variables back into Full DF
===

``` {r merge new variables}

full_order_data[1:1000,]

full_order_data <- full_order_data %>% 
  left_join(., avg_order_time, by = "user_id") %>% 
  left_join(., dow_freq_order, by = c("order_dow", "product_name")) %>% 
  left_join(., hour_freq_order, by = c("order_hour_of_day", "product_name")) %>% 
  left_join(., order_size, by = "order_id") %>% 
  left_join(., avg_order_size, by = "user_id") %>% 
  left_join(., reorder_counts, by = "product_name") %>% 
  left_join(., product_reorder_ratio, by = c("user_id", "product_name")) %>% 
  left_join(., reorder_rate_by_position, by = "add_to_cart_order")

full_order_data[1:1000,]

rm(hour_freq_order, order_size, product_count_per_user, product_reorder_ratio,
   reorder_counts, reorder_rate_by_position, avg_order_size, avg_order_time,
   dow_freq_order, products, aisles, depts)
```


## Correlation Matrix
===

```{r correlation matrix}
# Class of each variable
full_order_data %>% 
  sapply(class)

# Correlation Matrix of Variables
corr_matrix <- full_order_data %>% 
  mutate(order_dow_numeric = as.numeric(order_dow)) %>% 
  {cor(.[,-c(1, 2, 3, 5, 10, 11, 12)],
       use = "pairwise.complete.obs")}

# Visualize Corr Matrix
corrplot(corr_matrix, method="shade", type="lower", 
         title="Instacart Correlation Matrix",
         tl.cex = 0.75, tl.col = "black", tl.srt = 45,
         mar = c(0,0,1,0)) 
```

### Mostly uncorrelated variables -- some that are correlated tend to be because they were created by that variable (reordered / product_reorder_ratio). We do find that reorder_rate & add_to_cart_order have a negative relationship, alongside reorder_rate & order_size. Total_reorders has a positive correlation w/ product_count_day & product_count_hour.

===

# Model Building

### For our models -- we have a huge data set (32m observations). I tried running it with the full dataset initally, and it takes waaay too long as it is computationally very expensive on my macbook. Instead, I decided to first subset a random sample of the data so it moves down from 32m obs -> 8m obs, 1/4 of the full data set. I understand by doing this we lose a lot of our data, but it is still large enough where I don't expect this to be enough of a problem -- our data is still very low-dimensional.

### After subsetting the data, I then employed mini-batch training, which includes taking subsets of our shrunken data set defined as "batches" and apply regression models to this subset. We then iterate through this and update our model using each iteration. We run this algorithm until we hit the total number of batches, which is defined by the size of our training data set divided by the batch size (an arbitrary number we set -- not too high or else the model trains very slow). 

``` {r data splitting}
# data set is huge -- need to split it up randomly and rerun it -- use mini-batch training
# First -- create a subset of the original data set 1/4 of the size (32m -> 8m)
set.seed(444)
full_order_data <- full_order_data[sample(nrow(full_order_data), 8000000),]

# Second -- split original data set into training & test data (80/20)
train_index <- sample(1:nrow(full_order_data), 0.8*nrow(full_order_data))
train_data <- full_order_data[train_index,]
test_data <- full_order_data[-train_index,]

# Define batch size & num of batches
batch_size <- 50000
num_batches <- ceiling(nrow(train_data) / batch_size)

# Replace NA's with 0s in data, since they just correspond to # of days since previous order -- so 0 makes sense
# First, on the training data
train_data <- train_data %>% 
  mutate_all(~ifelse(is.na(.), 0, .))
# Test to see if it worked -- yes it did
sum(is.na(train_data)) # 0

# Now same on test data
test_data <- test_data %>% 
  mutate_all(~ifelse(is.na(.), 0, .))
# Again, we check -- looks good
sum(is.na(test_data)) # 0
```

## Logit
===

``` {r logit}
library(pROC)
library(class)
### Logit ###
#############

# Mini-batch training loop
for (i in 1:num_batches) {
  # Calculate start and end indices for the current batch
  start_index <- ((i - 1) * batch_size) + 1
  end_index <- min(i * batch_size, nrow(train_data))
  
  # Read the current batch of training data
  batch_data <- train_data[start_index:end_index, ]

  insta_logit <-  glm(reordered ~ order_number + order_dow + order_hour_of_day + days_since_prior_order +
        add_to_cart_order + avg_hour + avg_day + product_count_day + product_count_hour +
        order_size + avg_order_size + total_reorders + product_reorder_ratio + reorder_rate, 
      data = batch_data, family = "binomial")
}

# Summary statistics -- we find that order_number, order_dowTueday (relative to Saturday), days_since_prior_order, add_to_cart_order, avg_hour, order_size, avg_order_size, and product_reorder_rate are all statistically significant at all standard significance levels.
summary(insta_logit)

## Misclassificiation error rate
logit_preds <- predict(insta_logit, newdata = test_data, type = "response")
# Set A threshold value = 0.5
logit_threshold=ifelse(logit_preds>0.5,"1","0")
# Confusion Matrix
table(logit_threshold, test_data$reordered) 
# Test Error (diagonal/total)
mean(logit_threshold==test_data$reordered, na.rm = T) # logit model correctly predicts 86.37% of the time
# I.e. misclas error rate = 13.63%

# ROC Curve & AUC Value
roc_curve <- roc(test_data$reordered, logit_preds)
plot(roc_curve, main = "ROC Curve", col = "blue")
auc <- auc(roc_curve)
print(paste("AUC:", auc))
```


## LDA & QDA
===

``` {r LDA & QDA}
library(MASS)
### LDA & QDA ###
#################
set.seed(101)
for (i in 1:num_batches) {
  # Calculate start and end indices for the current batch
  start_index <- ((i - 1) * batch_size) + 1
  end_index <- min(i * batch_size, nrow(train_data))
  
  # Read the current batch of training data
  batch_data <- train_data[start_index:end_index, ]
  
  insta_LDA <- lda(reordered ~ order_number + order_dow + order_hour_of_day + days_since_prior_order + 
                     add_to_cart_order + avg_hour + avg_day + product_count_day + product_count_hour +
                     order_size + avg_order_size + total_reorders + product_reorder_ratio + reorder_rate,
                   data = batch_data)
  
  insta_QDA <- qda(reordered ~ order_number + order_dow + order_hour_of_day + days_since_prior_order +
                   add_to_cart_order + avg_hour + avg_day + product_count_day + product_count_hour +
                   order_size + avg_order_size + total_reorders + product_reorder_ratio + reorder_rate,
                 data = batch_data)
  }

lda.pred = predict(insta_LDA, newdata = test_data, type="response")
qda.pred = predict(insta_QDA, newdata = test_data, type="response")

# Plot of LDA model
plot(insta_LDA)

# Need to access column 'class' for LDA/QDA to create confusion matrix and calculate test error
table(lda.pred$class, test_data$reordered)
mean(lda.pred$class==test_data$reordered, na.rm = T) # LDA Correctly predicts 84.93% of time - misclassification error rate of 15.07%.

table(qda.pred$class, test_data$reordered) # QDA correctly predicts 83.00% of the time - misclas error rate = 17.00%. 
mean(qda.pred$class==test_data$reordered, na.rm = T)
```


## kNN

### Since kNN is a "lazy" learning method, a large data set takes way too long to train the model on. I tried this model initially on 10m observations, and it was gonna take over a day to run. So, we needed a much smaller subset of data -- we created a new subset on our 8m training data that we subsetted earlier, with a new subset of only 300,000 observations. This is a decrease in size by a factor of 26! Thus, our results should be taken with a grain of salt, since it might not be representative of our data and its findings, but it still should hopefully give us some meaningful insights into our results and give a sensible misclassification error rate.
===

``` {r kNN}
set.seed(18)
# kNN
 #only gonna use significant predictors from previous models in KNN model to reduce feature space
predictors <- c("order_number", "order_dow", "days_since_prior_order", 
    "add_to_cart_order", "avg_hour", "order_size", "avg_order_size", "product_reorder_ratio")

# Create a subset of data to use , since the larger the data points the more computationally expensive kNN becomes
knn_subset_data <- train_data[sample(nrow(train_data), 300000),]

knn_train_index <- sample(1:nrow(knn_subset_data), 0.8*nrow(knn_subset_data))
knn_train_data <- knn_subset_data[knn_train_index,]
knn_test_data <- knn_subset_data[-knn_train_index,]

# Experimented with different batch sizes
knn_batch_size = 2500
knn_num_batches <- ceiling(nrow(knn_train_data) / knn_batch_size)
  
## Find optimal K value for kNN using 10-fold CV
library(caret)
# Define range of k values
k_values <- c(5, 7, 9, 11)  # Example range of odds 5-11.

# Define the number of folds for cross-validation
num_folds <- 5  # Example: 10-fold cross-validation

# Define the control parameters for cross-validation
ctrl <- trainControl(method = "cv",  # Use cross-validation
                     number = num_folds)  # Number of folds

# Train kNN model using different values of k
knn_train <- train(as.factor(reordered) ~ order_number + order_dow + days_since_prior_order +
                     add_to_cart_order + avg_hour + order_size + avg_order_size +
                     product_reorder_ratio,  
                   data = knn_train_data,  # Training data
                   method = "knn",  # kNN method
                   trControl = ctrl,  # Cross-validation control
                   tuneGrid = data.frame(k = k_values))  # Grid of k values

# Print the results
print(knn_train) # best results from k = 1:5 -- 5 had highest accuracy (0.7162)
# Plot performance metric vs. k values
plot(knn_train)

## Loop through batches
for (i in 1:knn_num_batches) {
  # Get indices for the current batch
  start_index <- (i - 1) * knn_batch_size + 1
  end_index <- min(i * knn_batch_size, nrow(knn_train_data))
  
  # Read the current batch of training data
  knn_batch_data <- knn_train_data[start_index:end_index, ]
  
# Train kNN model on current batch
  knn_model <- knn(train = knn_batch_data[, predictors], 
                   test = knn_test_data[, predictors], 
                   cl = knn_batch_data$reordered, 
                   k = 5)
}

table(knn_model, knn_test_data$reordered) # KNN with K=5 confusion matrix
mean(knn_model==knn_test_data$reordered, na.rm = T) # Prediction accuracy of 68.83%.

```


## Classification Tree
===

``` {r classification tree}
library(tree)
### Classification Tree ###
###########################
set.seed(99)
for (i in 1:num_batches) {
  # Calculate start and end indices for the current batch
  start_index <- ((i - 1) * batch_size) + 1
  end_index <- min(i * batch_size, nrow(train_data))
  
  # Read the current batch of training data
  batch_data <- train_data[start_index:end_index, ]

  insta_tree <-  tree(as.factor(reordered) ~ order_number + order_dow + order_hour_of_day + 
                        days_since_prior_order + add_to_cart_order + avg_hour + avg_day + 
                        product_count_day + product_count_hour + order_size + avg_order_size + 
                        total_reorders + product_reorder_ratio + reorder_rate, 
                      data = batch_data)
}

# predicting values in our test split using our subsetted model
tree.pred <- predict(insta_tree, newdata = test_data, type = "class")
# confusion matrix
table(tree.pred, test_data$reordered) # correctly predicts 87.74% of the time. Misclas error rate of 12.26%.
mean(tree.pred==test_data$reordered)

# Cross-Validation on our Classification Tree
cv.insta <- cv.tree(insta_tree, FUN = prune.misclass) # using cv to determine optimal level of tree complexity
cv.insta$size # tree sizes
cv.insta$dev # tree size of 3 or 6 terminal nodes minimizes CV classification error 
cv.insta$k # alpha value of 0 -- no complexity penalty (unpruned tree minimizes our CV classification error)

# Plotting CV Trees
par(mfrow = c(1, 2))
plot(cv.insta$size , cv.insta$dev, type = "b") # Plot CV Classification error against tree size
min_dev = which.min(cv.insta$dev); min_dev # CV Classification error minimized at the first value (cv.insta$dev[1])
points(cv.insta$size[min_dev], cv.insta$dev[min_dev], col = "red", cex = 2, pch=20)

plot(cv.insta$k, cv.insta$dev, type = "b") # Plot CV Classification error against alpha values
points(0, cv.insta$dev[min_dev], col = "red", cex = 2, pch = 20)

# Pruning of our tree
prune.insta <- prune.misclass(insta_tree , best = 6)
plot(prune.insta); text(prune.insta , pretty = 0, cex = 0.5) # tree with 6 terminal nodes. We see the following vars: product_reorder_ratio & order_number

tree.pred2 <- predict(prune.insta, test_data, # predicting values in our test split using our subsetted model
                     type = "class")
table(tree.pred2, test_data$reordered) # same table as before -- test error of 87.74%
mean(tree.pred2==test_data$reordered)
```


## Random Forest
===

``` {r random forest}
library(randomForest)
### Random Forests ###
######################
set.seed(38)
# Tried with a smaller batch size to save time -- took 20 mins at 1/5 of the size -- over an hour for full size
rf_batch_size <- 10000

for (i in 1:num_batches) {
  # Calculate start and end indices for the current batch
  start_index <- ((i - 1) * rf_batch_size) + 1
  end_index <- min(i * rf_batch_size, nrow(train_data))
  
  # Read the current batch of training data
  batch_data <- train_data[start_index:end_index, ]

  insta_rf <-  randomForest(as.factor(reordered) ~ order_number + order_dow + order_hour_of_day + 
                              days_since_prior_order + add_to_cart_order + avg_hour + avg_day + 
                              product_count_day + product_count_hour +order_size + avg_order_size + 
                              total_reorders + product_reorder_ratio + reorder_rate, 
                            data = batch_data)
}

# Variable importance
importance(insta_rf)
varImpPlot(insta_rf)

rf_preds <- predict(insta_rf, newdata = test_data, 
                    type = "class")
table(rf_preds, test_data$reordered)
# Misclassification Error Rate (test error) of 11.77%. Correctly predicts 88.23% of the values.
mean(rf_preds==test_data$reordered)

```


## Boosting
### Because our memory is so full at this point, we can only run regression of size n=50000. If the memory clears up I can try running this regression again & seeing what results I can get at a much larger sample size, with more hypertuning of the parameters.
===

``` {r boosting}
library(tidyverse)
### Boosting ###
################
# Need to make a subset of our data to run on xgboost
set.seed(88)
xgboost_train_data <- train_data %>% 
  select(c(order_number, order_dow, order_hour_of_day, days_since_prior_order,
           add_to_cart_order, avg_hour, avg_day, product_count_day, product_count_hour,
           order_size, avg_order_size, total_reorders, product_reorder_ratio, reorder_rate,
           reordered)) %>%
  mutate(reordered = as.factor(reordered)) %>% 
  slice(sample(nrow(.), size = 50000)) 

xgboost_test_data <- test_data %>% 
  select(c(order_number, order_dow, order_hour_of_day, days_since_prior_order,
           add_to_cart_order, avg_hour, avg_day, product_count_day, product_count_hour,
           order_size, avg_order_size, total_reorders, product_reorder_ratio, reorder_rate,
           reordered)) %>%
  mutate(reordered = as.factor(reordered)) %>% 
  slice(sample(nrow(.), size = 12500))

head(xgboost_train_data)

library(caret)
library(xgboost)
# Tune parameters
grid_tune <- expand.grid(
  nrounds = c(500, 1000, 1500), # number of trees
  max_depth = c(2, 4, 6),
  eta = c(0.1, 0.3), # could try c(0.025, 0.05, 0.1, 0.3) # learning rate
  gamma = 0, 
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# Cross-validation
train_control <- trainControl(method = "cv",
                              number = 5,,
                              verboseIter = TRUE,
                              allowParallel = TRUE)

xgb_tune <- train(x = xgboost_train_data[,-15],
                  y = xgboost_train_data[,15],
                  trControl = train_control,
                  tuneGrid = grid_tune,
                  method = "xgbTree",
                  verbose = TRUE)

xgb_tune

best_params <- xgb_tune$bestTune # nrounds 500, depth 2, eta 0.1

# Final model -- grid
final_grid <- expand.grid(
  nrounds = best_params$nrounds,
  max_depth = best_params$max_depth,
  eta = best_params$eta,
  gamma = best_params$gamma, 
  colsample_bytree = best_params$colsample_bytree,
  min_child_weight = best_params$min_child_weight,
  subsample = best_params$subsample
)

xgb_final <- train(x = xgboost_train_data[,-15],
                  y = xgboost_train_data[,15],
                  trControl = train_control,
                  tuneGrid = final_grid,
                  method = "xgbTree",
                  verbose = TRUE)

xgb_final
# Misclassification error rate of 11.56%.
boost_preds <- predict(xgb_final, newdata = xgboost_test_data, type = "raw")
table(boost_preds, xgboost_test_data$reordered)
mean(boost_preds==xgboost_test_data$reordered) # Correctly predicts 88.44% of the time.

# Evaluate model
library(pROC)
roc_curve2 <- roc(xgboost_test_data$reordered, as.numeric(boost_preds))
plot(roc_curve2, main = "ROC Curve", col = "blue")
auc2 <- auc(roc_curve2)
print(paste("AUC:", auc2)) # AUC: 86.80
```

## SVM
===

``` {r svm}
set.seed(55)
# Create a subset of data to use
svm_subset_data <- train_data[sample(nrow(train_data), 10000),]

svm_train_index <- sample(1:nrow(svm_subset_data), 0.8*nrow(svm_subset_data))
svm_train_data <- svm_subset_data[svm_train_index,]
svm_test_data <- svm_subset_data[-svm_train_index,]

library(e1071)
# Find best hyperparameters for SVM model
tune.out <- tune(svm , reordered ~ order_number + order_dow + order_hour_of_day + days_since_prior_order +
                             add_to_cart_order + avg_hour + avg_day + product_count_day + product_count_hour +
                             order_size + avg_order_size + total_reorders + product_reorder_ratio + reorder_rate, 
                 data = svm_train_data,
                 kernel = "radial",
                 ranges = list(
                   cost = c(1, 10),
                   gamma = c(0.5, 1)
                   )
                 )

tune.out

final_hypers <- tune.out$best.parameters

# Support Vector Machines
svmfit <- svm(reordered ~ order_number + order_dow + order_hour_of_day + days_since_prior_order +
                             add_to_cart_order + avg_hour + avg_day + product_count_day + product_count_hour +
                             order_size + avg_order_size + total_reorders + product_reorder_ratio + reorder_rate, 
                data = svm_train_data, 
                kernel = "radial", 
                gamma = final_hypers$gamma, 
                cost = final_hypers$cost)

summary(svmfit)

svm_preds <- predict(svmfit, newdata = svm_test_data)

svm_class_preds <- ifelse(svm_preds > 0.5, 1, 0)

table(svm_class_preds, svm_test_data$reordered) # Correctly predicts 84.7% of the time. 
mean(svm_class_preds==svm_test_data$reordered) # Misclassification error rate of 15.3%
```




``` {r save workspace}
save.image()

```